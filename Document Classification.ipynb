{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required modules\n",
    "import os\n",
    "import nltk\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import random as rn\n",
    "from itertools import combinations\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "ipkRLagPbyFJ",
    "outputId": "0cffd542-0a4d-4316-f7bd-aae37d64fd0a"
   },
   "outputs": [],
   "source": [
    "#Extracting the class labels\n",
    "\n",
    "source='documents'\n",
    "data_files = os.listdir(source)\n",
    "class_list=[]\n",
    "for file in data_files:\n",
    "    label=file.split('_')[0]\n",
    "    class_list.append(label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wYskuxynbyFK",
    "outputId": "cd9da324-f381-4a62-e7f3-7a1df10ee0ce"
   },
   "outputs": [],
   "source": [
    "#Extracting Emails\n",
    "\n",
    "all_doc_emails_list=[]\n",
    "for file in data_files:\n",
    "   \n",
    "    file_read=open('documents/'+file,'r')\n",
    "    \n",
    "    emails_list=[]\n",
    "   \n",
    "    emails=[]\n",
    "    for line in file_read:\n",
    "        s=line.split(' ')\n",
    "        for word in s:\n",
    "            \n",
    "            if '@' in word:\n",
    "                emails.append(word)\n",
    "                word=word.lower()\n",
    "                after=word.split('@')[1]\n",
    "                words=after.split('.')\n",
    "                \n",
    "        \n",
    "                bad_words=[]\n",
    "                for word in words:\n",
    "                    \n",
    "                    if len(word)<=2:\n",
    "                        bad_words.append(word)\n",
    "                    if word=='com':\n",
    "                        bad_words.append(word)\n",
    "                words=set(words)\n",
    "                bad_words=set(bad_words)\n",
    "                email=words-bad_words\n",
    "                email=list(email)\n",
    "                emails_list=emails_list+email\n",
    "                line=line.replace(word,' ')\n",
    "                \n",
    "                \n",
    "    #Replacing Emails with ' '\n",
    "    for email in emails:\n",
    "        file_read=open('documents/'+file, \"rt\")\n",
    "        data=file_read.read()\n",
    "        data=data.replace(email,' ')\n",
    "        file_read.close()\n",
    "        #open the input file in write mode\n",
    "        file_read= open('documents/'+file, \"wt\")\n",
    "        #overrite the input file with the resulting data\n",
    "        file_read.write(data)\n",
    "        #close the file\n",
    "        file_read.close()\n",
    "        \n",
    "        \n",
    "\n",
    "    emails_list=set(emails_list)\n",
    "    emails_list=list(emails_list)\n",
    "    words_combined=''\n",
    "    for ele in emails_list:\n",
    "        words_combined=words_combined+ele+' '  \n",
    "\n",
    "   \n",
    "    all_doc_emails_list.append(words_combined)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5aCtzJKFbyFK",
    "outputId": "f48f7a7a-7471-4ada-d49a-7717f3d86ec8"
   },
   "outputs": [],
   "source": [
    "#Extracting Subjects\n",
    "\n",
    "subjects_list=[]\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    subject_list=[]\n",
    "    for line in file_read:\n",
    "        if 'Subject' in line:\n",
    "            pos=line.find('Subject:')\n",
    "            start=line.find(':',pos+8)\n",
    "            sentance=line[pos:]\n",
    "            subject=line[start+1:]\n",
    "            sent = subject.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            subject_list.append(sent)\n",
    "    subjects_list.append(subject_list[0])\n",
    "   \n",
    "    \n",
    "    #Replacing the sentance with ' '\n",
    "    file_read=open('documents/'+file, \"rt\")\n",
    "    data=file_read.read()\n",
    "    data=data.replace(sentance,' ')\n",
    "    file_read.close()\n",
    "    #open the input file in write mode\n",
    "    file_read= open('documents/'+file, \"wt\")\n",
    "    #overrite the input file with the resulting data\n",
    "    file_read.write(data)\n",
    "    #close the file\n",
    "    file_read.close()\n",
    "           \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "61tPhvzTbyFL",
    "outputId": "ba85d0f9-7359-49f7-e8a2-f6c2f95bfb64"
   },
   "outputs": [],
   "source": [
    "# 5 Removing From: Write to:\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "        if line.startswith('From:') or line.startswith('Write to:'):\n",
    "            \n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace(line,' ')\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1mY6QlKDbyFL",
    "outputId": "55d2ea6f-623c-499b-cba5-45a19dbd4087"
   },
   "outputs": [],
   "source": [
    "# 6 removing <>\n",
    "\n",
    "\n",
    "for file in data_files:\n",
    "    \n",
    "    file_read=open('documents/'+file,'r')\n",
    "    data=file_read.read()\n",
    "    file_read.close()\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "     \n",
    "        if '<' and '>' in line:\n",
    "            pos=line.find('<')\n",
    "            stop=line.find('>',pos)\n",
    "            bad_sent=line[pos:stop+1]\n",
    "           \n",
    "           \n",
    "            \n",
    "            data=data.replace(bad_sent,'')\n",
    "            \n",
    "    file_read.close()\n",
    "    #open the input file in write mode\n",
    "    file_read= open('documents/'+file, \"w\")\n",
    "    #overrite the input file with the resulting data\n",
    "    for line in data:\n",
    "        file_read.write(line)\n",
    "    #close the file\n",
    "    file_read.close()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zYZgTGhRbyFM",
    "outputId": "94c2fb0e-44e0-47c2-f9f4-0066ce1997e0"
   },
   "outputs": [],
   "source": [
    "# 7 Removing brackets\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "        if '(' and ')' in line:\n",
    "            pos=line.find('(')\n",
    "            stop=line.find(')',pos)\n",
    "            bad_sent=line[pos:stop+1]\n",
    "           \n",
    "            \n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace(bad_sent,'')\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()\n",
    "            \n",
    "        if '(' in line:\n",
    "            pos=line.find('(')\n",
    "            bad_sent=line[pos:]\n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace(bad_sent,'')\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()\n",
    "            \n",
    "        if ')' in line:\n",
    "            stop=line.find(')')\n",
    "            bad_sent=line[:stop+1]\n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace(bad_sent,'')\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bfZkqyREbyFM"
   },
   "outputs": [],
   "source": [
    "#8 Removing \\n,\\t etc.\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "            \n",
    "            sent = line.replace('\\t', ' ')\n",
    "            sent = sent.replace('\\n', ' ')\n",
    "            sent = sent.replace('\\\\', ' ')\n",
    "            sent=sent.replace('-',' ')\n",
    "            \n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace(line,sent)\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7-q6_0JQbyFM",
    "outputId": "64e03aa0-73ba-482c-beb4-23c8f687e9a0"
   },
   "outputs": [],
   "source": [
    "#9 Removing ':' \n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "            words=line.split()\n",
    "            for word in words:\n",
    "                if ':' in word:        \n",
    "                    file_read=open('documents/'+file, \"rt\")\n",
    "                    data=file_read.read()\n",
    "                    data=data.replace(word,'')\n",
    "                    file_read.close()\n",
    "                    #open the input file in write mode\n",
    "                    file_read= open('documents/'+file, \"wt\")\n",
    "                    #overrite the input file with the resulting data\n",
    "                    file_read.write(data)\n",
    "                    #close the file\n",
    "                    file_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0U3VCTd6RNhC"
   },
   "outputs": [],
   "source": [
    "#10 Decontracting\n",
    "\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "for file in data_files:\n",
    "    replacement=\"\"\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "       line = line.strip()\n",
    "       changes=decontracted(line)\n",
    "       replacement = replacement + changes + \"\\n\"\n",
    "\n",
    "    file_read.close()\n",
    "    # opening the file in write mode\n",
    "    fout = open('documents/'+file, \"w\")\n",
    "    fout.write(replacement)\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QQxDSeA6byFN",
    "outputId": "12cb3374-b65f-4673-b6d0-b29bde13f195"
   },
   "outputs": [],
   "source": [
    "# 11 Chunking\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    for line in file_read:\n",
    "        chunked = ne_chunk(pos_tag(word_tokenize(line)))\n",
    "\n",
    "        for chunk in chunked:\n",
    "          if hasattr(chunk, 'label'):\n",
    "            if chunk.label()=='PERSON':\n",
    "              Person= ' '.join(c[0] for c in chunk)\n",
    "\n",
    "              #Removing the person name\n",
    "              \n",
    "              file_read=open('documents/'+file, \"rt\")\n",
    "              data=file_read.read()\n",
    "              data=data.replace(Person,'')\n",
    "              file_read.close()\n",
    "              #open the input file in write mode\n",
    "              file_read= open('documents/'+file, \"wt\")\n",
    "              #overrite the input file with the resulting data\n",
    "              file_read.write(data)\n",
    "              #close the file\n",
    "              file_read.close()\n",
    "            \n",
    "            else:\n",
    "             NamedEntity=' '.join(c[0] for c in chunk)\n",
    "             Named_Entity='_'.join(c[0] for c in chunk)\n",
    "\n",
    "\n",
    "             # Adding _ to named entity \n",
    "              \n",
    "             file_read=open('documents/'+file, \"rt\")\n",
    "             data=file_read.read()\n",
    "             data=data.replace(NamedEntity,Named_Entity)\n",
    "             file_read.close()\n",
    "             #open the input file in write mode\n",
    "             file_read= open('documents/'+file, \"wt\")\n",
    "             #overrite the input file with the resulting data\n",
    "             file_read.write(data)\n",
    "             #close the file\n",
    "             file_read.close()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pidQMXy1QrgP"
   },
   "outputs": [],
   "source": [
    "# 13 Removing all the digits in the text file\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    replacement=\"\"\n",
    "    for line in file_read:\n",
    "      line.strip()\n",
    "      newline = re.sub(r'[0-9]+', '',line)\n",
    "      changes = line.replace(line,newline)\n",
    "      replacement = replacement + changes + \"\\n\"\n",
    "\n",
    "    file_read.close()\n",
    "    # opening the file in write mode\n",
    "    fout = open('documents/'+file, \"w\")\n",
    "    fout.write(replacement)\n",
    "    fout.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e6-e8f2YYuw_"
   },
   "outputs": [],
   "source": [
    "# 14 Removing _ from words\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    \n",
    "    for line in file_read:\n",
    "      line.strip()\n",
    "      words=line.split()\n",
    "      for word in words:\n",
    "        \n",
    "        if word[0]=='_' and word[-1]=='_':\n",
    "          fixed_word= re.sub('[^A-Za-z0-9]+', ' ',word)\n",
    "          \n",
    "          file_read=open('documents/'+file, \"rt\")\n",
    "          data=file_read.read()\n",
    "          data=data.replace(word,fixed_word)\n",
    "          file_read.close()\n",
    "          #open the input file in write mode\n",
    "          file_read= open('documents/'+file, \"wt\")\n",
    "          #overrite the input file with the resulting data\n",
    "          file_read.write(data)\n",
    "          #close the file\n",
    "          file_read.close()\n",
    "         \n",
    "        elif word[0]=='_':\n",
    "          fixed_word= re.sub('[^A-Za-z0-9]+', ' ',word)\n",
    "          \n",
    "          file_read=open('documents/'+file, \"rt\")\n",
    "          data=file_read.read()\n",
    "          data=data.replace(word,fixed_word)\n",
    "          file_read.close()\n",
    "          #open the input file in write mode\n",
    "          file_read= open('documents/'+file, \"wt\")\n",
    "          #overrite the input file with the resulting data\n",
    "          file_read.write(data)\n",
    "          #close the file\n",
    "          file_read.close()\n",
    "         \n",
    "        elif word[-1]=='_':\n",
    "          fixed_word= re.sub('[^A-Za-z0-9]+', ' ',word)\n",
    "          \n",
    "          file_read=open('documents/'+file, \"rt\")\n",
    "          data=file_read.read()\n",
    "          data=data.replace(word,fixed_word)\n",
    "          file_read.close()\n",
    "          #open the input file in write mode\n",
    "          file_read= open('documents/'+file, \"wt\")\n",
    "          #overwrite the input file with the resulting data\n",
    "          file_read.write(data)\n",
    "          #close the file\n",
    "          file_read.close()\n",
    "      \n",
    "      \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "b9giAXWwxm_N"
   },
   "outputs": [],
   "source": [
    "#Removing the word from word_word if len(word)<=2\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    \n",
    "    for line in file_read:\n",
    "      line.strip()\n",
    "      words=line.split()\n",
    "      for word in words:\n",
    "        if '_' in word:\n",
    "          word1=word.split('_')[0]\n",
    "          word2=word.split('_')[1]\n",
    "          \n",
    "          if len(word1)<=2:\n",
    "            \n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace(word1+'_',' ')\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overwrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()\n",
    "\n",
    "          if len(word2)<=2:\n",
    "            \n",
    "            file_read=open('documents/'+file, \"rt\")\n",
    "            data=file_read.read()\n",
    "            data=data.replace('_'+word2,' ')\n",
    "            file_read.close()\n",
    "            #open the input file in write mode\n",
    "            file_read= open('documents/'+file, \"wt\")\n",
    "            #overwrite the input file with the resulting data\n",
    "            file_read.write(data)\n",
    "            #close the file\n",
    "            file_read.close()\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "msA0uYMhzDDl"
   },
   "outputs": [],
   "source": [
    "#Lowercase\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    \n",
    "    for line in file_read:\n",
    "      \n",
    "      low_line=line.lower()\n",
    "      file_read=open('documents/'+file, \"rt\")\n",
    "      data=file_read.read()\n",
    "      data=data.replace(line,low_line)\n",
    "      file_read.close()\n",
    "      #open the input file in write mode\n",
    "      file_read= open('documents/'+file, \"wt\")\n",
    "      #overwrite the input file with the resulting data\n",
    "      file_read.write(data)\n",
    "      #close the file\n",
    "      file_read.close()\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aDtDydOljikE"
   },
   "outputs": [],
   "source": [
    "# Removing the words of len<=2 and len>=15\n",
    "\n",
    "for file in data_files:\n",
    "    \n",
    "    file_read=open('documents/'+file,'r')\n",
    "    \n",
    "    replacement=\"\"\n",
    "    for line in file_read:\n",
    "      bad_words=[]\n",
    "      line.strip()\n",
    "      words=line.split()\n",
    "      \n",
    "      for word in words:\n",
    "        if len(word)<=2:\n",
    "           \n",
    "           bad_words.append(word)\n",
    "\n",
    "        if len(word)>=15:\n",
    "           bad_words.append(word)\n",
    "      for word in bad_words:\n",
    "        words.remove(word)\n",
    "      \n",
    "      changes =' '.join(words)\n",
    "     \n",
    "      replacement = replacement + changes + \"\\n\"\n",
    "    \n",
    "    \n",
    "    file_read.close()\n",
    "    # opening the file in write mode\n",
    "    fout = open('documents/'+file, \"w\")\n",
    "    fout.write(replacement)\n",
    "    fout.close()\n",
    "\n",
    "          \n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "S66D-gNz0rKP"
   },
   "outputs": [],
   "source": [
    "# Keeping only alphabets\n",
    "\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "\n",
    "    replacement=\"\"\n",
    "    for line in file_read:\n",
    "      \n",
    "      line.strip()\n",
    "      changes = re.sub('[^A-Za-z_]+', ' ',line)\n",
    "      replacement = replacement + changes + \"\\n\"\n",
    "\n",
    "    file_read.close()\n",
    "    # opening the file in write mode\n",
    "    fout = open('documents/'+file, \"w\")\n",
    "    fout.write(replacement)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VkIQhYKjtKf7"
   },
   "outputs": [],
   "source": [
    "#Extracting text data\n",
    "text_list=[]\n",
    "for file in data_files:\n",
    "    file_read=open('documents/'+file,'r')\n",
    "    data=file_read.read()\n",
    "    text_list.append(data)\n",
    "    file_read.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18828"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Data Frame\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(list(zip(subjects_list,text_list,all_doc_emails_list,class_list)),columns =['Subjects','Pre-processed Text','Emails','Class_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subjects                                            Political Atheists \n",
       "Pre-processed Text    article now along comes and says here objectiv...\n",
       "Emails                       edu caltech cco gap sgi edu>, wpd solntze \n",
       "Class_Label                                                 alt.atheism\n",
       "Name: 56, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the columns into one\n",
    "\n",
    "Text_Data_Combined=data['Subjects']+' '+data['Pre-processed Text']+' '+data['Emails']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' KORESH IS GOD  mathew the latest news seems that will give himself once finished writing sequel the bible article writing the seven seals something along those lines already written the first the seven which was around pages and has handed over assistant for proofreading would expect any decent messiah have built in spellchecker maybe will come with one heard had asked the fbi provide him with word processor does anyone know has requested that wordperfect wp was written the theological implications requesting wp are profound darin will president for food \\n \\n scubed mantis uk>\\n psilink com\\n com> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_Data_Combined[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding the Class labels\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "Categorical_Class_Labels= ohe.fit_transform(data[['Class_Label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18828, 20)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorical_Class_Labels=Categorical_Class_Labels.toarray()\n",
    "Categorical_Class_Labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Text_Data_Combined,Categorical_Class_Labels, test_size=0.25, random_state=42,stratify=Categorical_Class_Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81553\n"
     ]
    }
   ],
   "source": [
    "#Word_Index\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "vocab_size=len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing\n",
    "\n",
    "sequences= tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "#Padding\n",
    "\n",
    "word_train_padded= pad_sequences(sequences,padding='pre',maxlen=250,truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list=[]\n",
    "for sequence in sequences:\n",
    "    length=len(sequence)\n",
    "    length_list.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.,  64., 109., 190.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequence Length distribution\n",
    "\n",
    "np.percentile(length_list,np.arange(0,100,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(length_list,85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14121, 250)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_train_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing\n",
    "\n",
    "sequences= tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#Padding\n",
    "\n",
    "word_test_padded= pad_sequences(sequences,padding='pre',maxlen=250,truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Embedding\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('GloVe/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Matrix\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81553, 100)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Callbacks\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,validation_data):\n",
    "      self.x_test = validation_data[0]\n",
    "      self.y_test= validation_data[1]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': [],'val_micro_F1': [],'AUC_Score': []}\n",
    "        # Creating a list for learning rates\n",
    "        self.lr=[]\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        true_positives=0\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['accuracy'].append(logs.get('accuracy'))\n",
    "\n",
    "        #Appending learning rate in the list lr\n",
    "        self.lr.append(changeLearningRate(epoch))\n",
    "        \n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_accuracy', -1) != -1:\n",
    "            self.history['val_accuracy'].append(logs.get('val_accuracy'))\n",
    "        \n",
    "       # Predicting the x_test and extracting the probability scores of Class 1\n",
    "     \n",
    "        y_pred= self.model.predict(self.x_test)\n",
    "        y_pred_pos_proba=y_pred[:,1]\n",
    "\n",
    "        # Sorting the probability scores in descending order for AUC \n",
    "        \n",
    "        proba=sorted(list(y_pred_pos_proba),reverse=True)\n",
    "        TPR_lst=[]\n",
    "        FPR_lst=[]\n",
    "        \n",
    "        # Calculating TPR and FPR for each probability thershold.\n",
    "        \n",
    "        for thershold in proba:\n",
    "             y_pred_auc=[]\n",
    "             for i in y_pred_pos_proba:\n",
    "                 if i<thershold:\n",
    "                     y_pred_auc.append(0)\n",
    "                 else:\n",
    "                     y_pred_auc.append(1)\n",
    "\n",
    "             y_pred_df=pd.DataFrame({'y_pred_auc':y_pred_auc})\n",
    "             y_test_df=pd.DataFrame({'y_test':y_test[:,1]})\n",
    "             new_df_a=pd.concat([y_test_df,y_pred_df],axis=1)   \n",
    "             new_df_a_array=np.array(new_df_a)\n",
    "             TP=TN=FN=FP=0\n",
    "             for i in new_df_a_array:\n",
    "                 if i[0]==i[1]:\n",
    "                     if i[0]==1:\n",
    "                         TP=TP+1\n",
    "                     else:\n",
    "                         TN=TN+1\n",
    "                 elif i[0]!=i[1]:\n",
    "                     if i[0]==1:\n",
    "                         FN=FN+1\n",
    "                     else:\n",
    "                         FP=FP+1\n",
    "             TPR=TP/(TP+FN)\n",
    "             FPR=FP/(TN+FP)\n",
    "             TPR_lst.append(TPR)\n",
    "             FPR_lst.append(FPR)\n",
    "        \n",
    "        TPR_array=np.array(TPR_lst)\n",
    "        FPR_array=np.array(FPR_lst)\n",
    "        \n",
    "        # Calculating the area under the curve.\n",
    "        \n",
    "        AUC=np.trapz(TPR_array,FPR_array)\n",
    "\n",
    "        # Finding the label using the probability scores\n",
    "\n",
    "        y_label_pred=np.argmax(y_pred,axis=1)\n",
    "        y_test_pos=y_test[:,1]\n",
    "\n",
    "        # Calculating TP,FP,FN for micro F1 score\n",
    "        \n",
    "        TP=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "\n",
    "        for i in range(len(y_test_pos)):\n",
    "          if y_label_pred[i]==y_test_pos[i]:\n",
    "            if y_label_pred[i]==1:\n",
    "              TP=TP+1\n",
    "          else:\n",
    "            if y_test_pos[i]==0:\n",
    "              FP=FP+1\n",
    "            else:\n",
    "              FN=FN+1\n",
    "        \n",
    "        \n",
    "\n",
    "        #micro_F1 Score \n",
    "        \n",
    "        micro_F1_score=TP/(TP+0.5*(FP+FN))\n",
    "\n",
    "        #AUC Score\n",
    "        \n",
    "        AUC_score=AUC\n",
    "        self.history['val_micro_F1'].append(micro_F1_score)\n",
    "        self.history['AUC_Score'].append(AUC_score)\n",
    "        print('val_micro_F1: ',np.round(micro_F1_score,5),'AUC Score: ',AUC_score)\n",
    "       \n",
    "\n",
    "\n",
    "history_own=LossHistory(validation_data=[word_test_padded,y_test])  \n",
    "\n",
    "# Saving the model ###########################################################################################################\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max') \n",
    "\n",
    "# Deacying the learning rate #################################################################################################\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def changeLearningRate(epoch):\n",
    "    \n",
    "    if epoch<2:\n",
    "      changed=0.01\n",
    "    \n",
    "    \n",
    "    if epoch>=2:\n",
    "      \n",
    "      # Condition1\n",
    "      if history_own.history['val_accuracy'][epoch-2]>history_own.history['val_accuracy'][epoch-1]:\n",
    "        \n",
    "        changed=history_own.lr[epoch-1]-0.1*history_own.lr[epoch-1]\n",
    "      \n",
    "      # Condition2\n",
    "      elif (epoch+1)%3==0:\n",
    "        changed=history_own.lr[epoch-1]-0.05*history_own.lr[epoch-1]\n",
    "\n",
    "      else:\n",
    "        changed=history_own.lr[epoch-1]\n",
    "\n",
    "    return changed\n",
    "\n",
    "lrschedule = LearningRateScheduler(changeLearningRate, verbose=1)\n",
    "\n",
    "\n",
    "# Nan Values ###############################################################################################################\n",
    "\n",
    "class TerminateNaN(tf.keras.callbacks.Callback):\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        # If weights have NaN values\n",
    "        weights=self.model.get_weights()\n",
    "        print(type(weights))\n",
    "        if weights is not None:\n",
    "          for weight in weights:\n",
    "              if np.any(np.isnan(weight)):\n",
    "                print('Invalid weights and terminated at epoch{}'.format(epoch))\n",
    "                self.model.stop_training=True\n",
    "\n",
    "        # If loss have NaN values  \n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True\n",
    "\n",
    "terminate_training=TerminateNaN()\n",
    "\n",
    "#Early Stopping ###############################################################################################################\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0mwdtcvYv1X"
   },
   "source": [
    "### Model-1: Using 1D convolutions with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 250, 100)     8155300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 246, 64)      32064       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 245, 64)      38464       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 244, 64)      44864       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 735, 64)      0           conv1d[0][0]                     \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 367, 64)      0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 364, 32)      8224        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 363, 32)      10272       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 362, 32)      12320       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1089, 32)     0           conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 544, 32)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 539, 16)      3088        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8624)         0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8624)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          1104000     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           2580        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,411,176\n",
      "Trainable params: 1,255,876\n",
      "Non-trainable params: 8,155,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(250,))\n",
    "Embedding_Layer=Embedding(vocab_size,100,weights=[embedding_matrix],input_length=250,trainable=False)(inputs)\n",
    "conv1 = Conv1D(64, kernel_size=5, activation='relu')(Embedding_Layer)\n",
    "conv2 = Conv1D(64, kernel_size=6, activation='relu')(Embedding_Layer)\n",
    "conv3 = Conv1D(64, kernel_size=7, activation='relu')(Embedding_Layer)\n",
    "merge1=tf.keras.layers.Concatenate(axis=1)([conv1, conv2,conv3])\n",
    "pool1 = MaxPooling1D()(merge1)\n",
    "conv4 = Conv1D(32, kernel_size=4,activation='relu')(pool1)\n",
    "conv5 = Conv1D(32, kernel_size=5,activation='relu')(pool1)\n",
    "conv6 = Conv1D(32, kernel_size=6,activation='relu')(pool1)\n",
    "merge2=tf.keras.layers.Concatenate(axis=1)([conv4,conv5,conv6])\n",
    "pool2 = MaxPooling1D()(merge2)\n",
    "conv7 = Conv1D(16, kernel_size=6,activation='relu')(pool2)\n",
    "Flatten1=Flatten()(conv7)\n",
    "Dropout=tf.keras.layers.Dropout(0.2)(Flatten1)\n",
    "Dense1=tf.keras.layers.Dense(128,activation=\"relu\")(Dropout)\n",
    "output=tf.keras.layers.Dense(20,activation=\"softmax\")(Dense1)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "print(model.summary())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/10\n",
      "  1/883 [..............................] - ETA: 0s - loss: 3.0306 - accuracy: 0.0625WARNING:tensorflow:From D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "882/883 [============================>.] - ETA: 0s - loss: 2.3516 - accuracy: 0.2107val_micro_F1:  0.03183 AUC Score:  0.8943915291237075\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.35564, saving model to model_save\\weights-01-0.3556.hdf5\n",
      "883/883 [==============================] - 102s 116ms/step - loss: 2.3512 - accuracy: 0.2109 - val_loss: 1.7877 - val_accuracy: 0.3556\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 1.5044 - accuracy: 0.4611val_micro_F1:  0.0066 AUC Score:  0.9212594215083263\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.35564 to 0.54366, saving model to model_save\\weights-02-0.5437.hdf5\n",
      "883/883 [==============================] - 102s 115ms/step - loss: 1.5040 - accuracy: 0.4612 - val_loss: 1.3256 - val_accuracy: 0.5437\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0095.\n",
      "Epoch 3/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 1.1054 - accuracy: 0.6132val_micro_F1:  0.02975 AUC Score:  0.9253405386669027\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.54366 to 0.64075, saving model to model_save\\weights-03-0.6407.hdf5\n",
      "883/883 [==============================] - 104s 118ms/step - loss: 1.1053 - accuracy: 0.6132 - val_loss: 1.0467 - val_accuracy: 0.6407\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0095.\n",
      "Epoch 4/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 0.8370 - accuracy: 0.7140val_micro_F1:  0.03677 AUC Score:  0.9470846792630943\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.64075 to 0.68026, saving model to model_save\\weights-04-0.6803.hdf5\n",
      "883/883 [==============================] - 101s 115ms/step - loss: 0.8370 - accuracy: 0.7140 - val_loss: 0.9432 - val_accuracy: 0.6803\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0095.\n",
      "Epoch 5/10\n",
      "883/883 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.7750val_micro_F1:  0.05741 AUC Score:  0.9473003967727186\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.68026 to 0.71043, saving model to model_save\\weights-05-0.7104.hdf5\n",
      "883/883 [==============================] - 106s 120ms/step - loss: 0.6513 - accuracy: 0.7750 - val_loss: 0.8985 - val_accuracy: 0.7104\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009025.\n",
      "Epoch 6/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 0.4868 - accuracy: 0.8304 ETA: 0s - loss: 0.4val_micro_F1:  0.04951 AUC Score:  0.9506301901264068\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.71043 to 0.71383, saving model to model_save\\weights-06-0.7138.hdf5\n",
      "883/883 [==============================] - 102s 116ms/step - loss: 0.4867 - accuracy: 0.8304 - val_loss: 0.9026 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009025.\n",
      "Epoch 7/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8660val_micro_F1:  0.07376 AUC Score:  0.9537396566219745\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.71383 to 0.71617, saving model to model_save\\weights-07-0.7162.hdf5\n",
      "883/883 [==============================] - 104s 117ms/step - loss: 0.3902 - accuracy: 0.8660 - val_loss: 0.9667 - val_accuracy: 0.7162\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.009025.\n",
      "Epoch 8/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8929val_micro_F1:  0.06719 AUC Score:  0.9509814224818207\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.71617 to 0.72573, saving model to model_save\\weights-08-0.7257.hdf5\n",
      "883/883 [==============================] - 103s 116ms/step - loss: 0.3138 - accuracy: 0.8927 - val_loss: 1.0606 - val_accuracy: 0.7257\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00857375.\n",
      "Epoch 9/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9187val_micro_F1:  0.06793 AUC Score:  0.950316754428662\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.72573\n",
      "883/883 [==============================] - 102s 116ms/step - loss: 0.2359 - accuracy: 0.9188 - val_loss: 1.1270 - val_accuracy: 0.7223\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.007716374999999999.\n",
      "Epoch 10/10\n",
      "882/883 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9493val_micro_F1:  0.04482 AUC Score:  0.9443163045562488\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.72573\n",
      "883/883 [==============================] - 101s 114ms/step - loss: 0.1490 - accuracy: 0.9492 - val_loss: 1.2595 - val_accuracy: 0.7240\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x278d1c9cbe0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1,momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ \n",
    "log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "\n",
    "model.fit(word_train_padded,y_train,epochs=10, validation_data=(word_test_padded,y_test), batch_size=16, callbacks=[lrschedule,history_own,checkpoint,earlystop,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 4s 28ms/step - loss: 1.2595 - accuracy: 0.7240\n"
     ]
    }
   ],
   "source": [
    "scores=model.evaluate(x=word_test_padded,y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot \n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\anaconda_and_libraries\\envs\\ai\\lib\\site-packages (22.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\anaconda_and_libraries\\envs\\ai\\lib\\site-packages (2.3.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000014C83C7CC10>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/tensorflow/\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 435, in _error_catcher\n",
      "    yield\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 516, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\http\\client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\http\\client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 341, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "                                             3.4/444.1 MB 20.8 kB/s eta 5:52:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 348, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 97, in _iter_built_with_inserted\n",
      "    candidate = func()\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 215, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 291, in __init__\n",
      "    super().__init__(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 161, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 230, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 302, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 428, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 473, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 155, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 96, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 146, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 573, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 538, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"D:\\Anaconda_and_Libraries\\envs\\AI\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 440, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "#Plotting the Model \n",
    "import pydot\n",
    "import graphviz\n",
    "tf.keras.utils.plot_model(model, to_file=\"my_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cg4L1V4Yv1d"
   },
   "source": [
    "### Model-2 : Using 1D convolutions with character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index={'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'_':26,' ':27}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_char=len(char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequencing all the train text files\n",
    "\n",
    "Char_Sequences=[]\n",
    "\n",
    "for text in X_train:\n",
    "    sequence=[]\n",
    "    for char in text:\n",
    "        index=char_index.get(char,0)\n",
    "        sequence.append(index)\n",
    "    Char_Sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding\n",
    "\n",
    "char_train_padded= pad_sequences(Char_Sequences,padding='pre',maxlen=1100,truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list=[]\n",
    "for sequence in Char_Sequences:\n",
    "    length=len(sequence)\n",
    "    length_list.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2029.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(length_list,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  20.,  381.,  650., 1137.])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(length_list,np.arange(0,100,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing Test text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequencing all the test text files\n",
    "\n",
    "Char_test_Sequences=[]\n",
    "\n",
    "for text in X_test:\n",
    "    sequence=[]\n",
    "    for char in text:\n",
    "        index=char_index.get(char,0)\n",
    "        sequence.append(index)\n",
    "    Char_test_Sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding\n",
    "\n",
    "char_test_padded= pad_sequences(Char_test_Sequences,padding='pre',maxlen=1100,truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 94 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Embedding\n",
    "\n",
    "\n",
    "char_embeddings_index = dict()\n",
    "f = open('char_embedding/glove.840B.300d-char.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    char_embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(char_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Matrix\n",
    "\n",
    "char_embedding_matrix = zeros((len(char_index),300))\n",
    "for char, i in char_index.items():\n",
    "    embedding_vector = char_embeddings_index.get(char)\n",
    "    if embedding_vector is not None:\n",
    "        char_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 300)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_size=len(char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 1100)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, 1100, 300)         8400      \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 1098, 128)         115328    \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 1095, 128)         65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 547, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 543, 64)           41024     \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 538, 64)           24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 269, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 17216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 17216)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               2203776   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,461,412\n",
      "Trainable params: 2,453,012\n",
      "Non-trainable params: 8,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = Input(shape=((1100,)))\n",
    "Embedding_Layer=Embedding(char_vocab_size,300,weights=[char_embedding_matrix],input_length=1100,trainable=False)(inputs)\n",
    "conv1 = Conv1D(128, kernel_size=3,activation='sigmoid')(Embedding_Layer)\n",
    "conv2 = Conv1D(128, kernel_size=4, activation='sigmoid')(conv1)\n",
    "pool1 = MaxPooling1D()(conv2)\n",
    "conv4 = Conv1D(64, kernel_size=5, activation='sigmoid')(pool1)\n",
    "conv5 = Conv1D(64, kernel_size=6, activation='sigmoid')(conv4)\n",
    "pool2 = MaxPooling1D()(conv5)\n",
    "Flatten1=Flatten()(pool2)\n",
    "Dropout=tf.keras.layers.Dropout(0.2)(Flatten1)\n",
    "Dense1=tf.keras.layers.Dense(128, activation=\"relu\")(Dropout)\n",
    "output=tf.keras.layers.Dense(20,activation=\"softmax\")(Dense1)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "print(model.summary())\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/10\n",
      "  2/883 [..............................] - ETA: 8:22 - loss: 119.0318 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1928s vs `on_train_batch_end` time: 0.9483s). Check your callbacks.\n",
      "883/883 [==============================] - ETA: 0s - loss: 3.4196 - accuracy: 0.0500\n",
      "Epoch 00001: val_accuracy did not improve from 0.71787\n",
      "883/883 [==============================] - 165s 187ms/step - loss: 3.4196 - accuracy: 0.0500 - val_loss: 2.9922 - val_accuracy: 0.0527\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/10\n",
      "883/883 [==============================] - ETA: 0s - loss: 2.9940 - accuracy: 0.0507\n",
      "Epoch 00002: val_accuracy did not improve from 0.71787\n",
      "883/883 [==============================] - 159s 180ms/step - loss: 2.9940 - accuracy: 0.0507 - val_loss: 2.9915 - val_accuracy: 0.0525\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0095.\n",
      "Epoch 3/10\n",
      "883/883 [==============================] - ETA: 0s - loss: 2.9933 - accuracy: 0.0506\n",
      "Epoch 00003: val_accuracy did not improve from 0.71787\n",
      "883/883 [==============================] - 159s 181ms/step - loss: 2.9933 - accuracy: 0.0506 - val_loss: 2.9916 - val_accuracy: 0.0529\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0095.\n",
      "Epoch 4/10\n",
      "883/883 [==============================] - ETA: 0s - loss: 2.9933 - accuracy: 0.0523\n",
      "Epoch 00004: val_accuracy did not improve from 0.71787\n",
      "883/883 [==============================] - 159s 180ms/step - loss: 2.9933 - accuracy: 0.0523 - val_loss: 2.9934 - val_accuracy: 0.0523\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0095.\n",
      "Epoch 5/10\n",
      "883/883 [==============================] - ETA: 0s - loss: 2.9932 - accuracy: 0.0539\n",
      "Epoch 00005: val_accuracy did not improve from 0.71787\n",
      "883/883 [==============================] - 159s 180ms/step - loss: 2.9932 - accuracy: 0.0539 - val_loss: 2.9922 - val_accuracy: 0.0525\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc8f657220>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(char_train_padded,y_train,epochs=10, validation_data=(char_test_padded,y_test), batch_size=16, callbacks=[lrschedule,checkpoint,earlystop,tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification Assignment.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
